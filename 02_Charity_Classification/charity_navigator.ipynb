{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<center> <h1> <span style='color:#C7B097'> AltaML Hackathon 2022 </span> </h2> </center>\n",
        "<center> <h2> <span style='color:#98AFC7'> Charity Classification </span> </h1> </center>\n",
        "<center> <img src=\"https://altaml.com/media/ul4ibm45/logo-for-dark.png?mode=pad&width=200&height=70&format=webp&quality=100\" alt=\"altaml\" style=\"width:200px;\"> </center>\n",
        "<center> <img src=\"https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31\" alt=\"microsoft\" style=\"width:200px;\"> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n",
        "<center> <h2> <span style='color:#C7B097'> Table of Content </span> </h2> </center>\n",
        "\n",
        "* [1 - Introduction](#1-introduction)\n",
        "  * [1.1 - Import libraries](#11---import-libraries)\n",
        "    * [1.1.1 - Configurations](#111---configurations)\n",
        "* [2 - Data](#2-data)\n",
        "  * [2.1 - Charity Navigator Dataset](#21-charity-navigator-dataset)\n",
        "    * [2.1.1 - Data Preparation](#211---data-preparation)\n",
        "    * [2.1.2 - Data Wrangling](#212---data-wrangling)\n",
        "    * [2.1.3 - TF-IDF with n-grams](#213---tf-idf-with-n-grams)\n",
        "    * [2.1.4 - Feature Selection](#214---feature-selection)\n",
        "    * [2.1.5 - Oversampling and Undersampling for inbalanced data](#215---oversampling-and-undersampling-for-inbalanced-data)\n",
        "* [3 - Classification](#3---classification)\n",
        "  * [3.1 - Logistic Regression](#31---logistic-regression)\n",
        "  * [3.2 - Gradient Boosting XGBoost](#32---gradient-boosting-xgboost)\n",
        "    * [3.2.1 - Feature Importance (First 30 features)](#321---feature-importance-first-30-features)\n",
        "* [4 - Final Results](#4---final-results)\n",
        "* [5 - Test loading the pickle file](#5-test-loading-the-pickle-file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1 - Introduction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Description: \n",
        "\n",
        "This project aims to classify the charities among 10 different categories. The dataset have information of 8400 different US charities as rated by CharityNavigator.org\n",
        "\n",
        "Possible categories for each charity:\n",
        "\n",
        "1. Animals\n",
        "2. Arts, Culture, Humanities\n",
        "3. Community Development\n",
        "4. Education\n",
        "5. Environment\n",
        "6. Health\n",
        "7. Human Services\n",
        "8.  Human and Civil Rights\n",
        "9.  Religion\n",
        "10. Research and Public Policy\n",
        "\n",
        "\n",
        "#### Dataset:\n",
        "Link : \n",
        "<a href=\"https://www.kaggle.com/datasets/katyjqian/charity-navigator-scores-expenses-dataset?resource=download\"> Charity Navigator Dataset</a>\n",
        "\n",
        "The data is a public service of Charity Navigator, but the data is likely owned by individual charities. Charity Navigator collects this data. This data was webscraped in May 2019 but uses rating details mostly from 2017"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 - Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033319634
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import os\n",
        "\n",
        "# NLP\n",
        "import nltk.corpus\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Visualization\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pylab import rcParams\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "\n",
        "# Features Extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.linear_model import LogisticRegression # Logistic Regression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree # Decision Tree\n",
        "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
        "from xgboost import XGBClassifier, plot_importance # Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting\n",
        "from sklearn.neural_network import MLPClassifier # Neural Networks\n",
        "from sklearn.neighbors import KNeighborsClassifier # KNeighbors\n",
        "from sklearn.svm import SVC # Support Vector Machine \n",
        "from sklearn.model_selection import train_test_split # Split the data\n",
        "import pickle # Format to save the ML model\n",
        "\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (f1_score, accuracy_score, recall_score, \n",
        "precision_score, classification_report)\n",
        "from sklearn.metrics import make_scorer \n",
        "\n",
        "# Inbalance Treatment\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Utilities functions\n",
        "import utils\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1.1 - Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show multiple outputs at the same cell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "warnings.filterwarnings(\"ignore\") # Ignore warnings\n",
        "plt.style.use('fivethirtyeight')\n",
        "rcParams['figure.figsize'] = 16, 6 # Set the standard plot size\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = make_scorer(accuracy_score)\n",
        "recall = make_scorer(recall_score, average='macro')\n",
        "precision = make_scorer(precision_score, average='macro')\n",
        "f1 = make_scorer(f1_score, average='macro')\n",
        "os.makedirs('outputs', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "smote = True\n",
        "metric = accuracy #f1 # Metric used for optimization\n",
        "metric_name = 'Accuracy' #'F1'\n",
        "search_params = True # Search for the best parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 - Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 - Charity Navigator Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[id1]: # \"Accountability & Transparency Score - %\"\n",
        "[id2]: # \"Charity class\"\n",
        "[id3]: # \"Mission & Description.\"\n",
        "[id4]: # \"ID number.\"\n",
        "[id5]: # \"Total Expenses in (Program , Funding ,Administrative).\"\n",
        "[id6]: # \"Administrative Expenses Percentage (of total expenses)%.\"\n",
        "[id7]: # \"Funding Efficiency in (amount spent to raise 1 in donations).\"\n",
        "[id8]: # \"Funding Expenses Percentage (of total expenses).\"\n",
        "[id9]: # \"Program Expenses Percentage (of total expenses).\"\n",
        "[id10]: # \"Financial Score (out of 100).\"\n",
        "[id11]: # \"Name of Leader.\"\n",
        "[id12]: # \"Compensation of Leader in.\"\n",
        "[id13]: # \"Compensation of Leader Percentage.\"\n",
        "[id14]: # \"Tagline.\"\n",
        "[id15]: # \"Name of Charity.\"\n",
        "[id16]: # \"Total Revenue.\"\n",
        "[id17]: # \"Overall Score (out of 100).\"\n",
        "[id18]: # \"State.\"\n",
        "[id19]: # \"Subcategory.\"\n",
        "[id20]: # \"Size of Charity (based on Total Expenses).\"\n",
        "[id21]: # \"Program Expenses in (amount spent on program & services it delivers).\"\n",
        "[id22]: # \"Funding Expenses in (amount spent on raising money).\"\n",
        "[id23]: # \"Administrative Expenses in $ (amount spent on overhead, staff, meeting costs).\"\n",
        "\n",
        "| Index | Feature (Input) | Short Description | Unit/Format |\n",
        "|:-------:|:-----------------|:------------------------------------------------------|:-------------------------------------|\n",
        "| 1 | [`ascore`][id1] | Accountability & Transparency Score | % |\n",
        "| 2 | [`category`][id2] | Charity class (`10 categories that will be used as output`) | dimensionless |\n",
        "| 3 | [`description`][id3] | Mission & Description (`Used as the only input`) | dimensionless |\n",
        "| 4 | [`EIN`][id4] | ID number | dimensionless |\n",
        "| 5 | [`tot_exp`][id5] | Total Expenses in (Program , Funding ,Administrative) | dimensionless |\n",
        "| 6 | [`admin_exp_p`][id6] | Administrative Expenses Percentage (of total expenses) | % |\n",
        "| 7 | [`fund_eff`][id7] | Funding Efficiency in (amount spent to raise 1 in donations) | dimensionless |\n",
        "| 7 | [`fund_exp_p`][id8] | Funding Expenses Percentage (of total expenses) | dimensionless |\n",
        "| 8 | [`program_exp_p`][id9] | Program Expenses Percentage (of total expenses) | % |\n",
        "| 9 | [`fscore`][id10] | Financial Score (out of 100) | dimensionless |\n",
        "| 10 | [`leader`][id11] | Name of Leader | dimensionless |\n",
        "| 11 | [`leader_comp`][id12] | Compensation of Leader in | dimensionless |\n",
        "| 12 | [`leader_comp_p`][id13] | Compensation of Leader Percentage | dimensionless |\n",
        "| 12 | [`motto`][id14] | Tagline | dimensionless |\n",
        "| 13 | [`name`][id15] | Name of Charity | dimensionless |\n",
        "| 14 | [`tot_rev`][id16] | Total Revenue | $ |\n",
        "| 15 | [`score`][id17] | Overall Score (out of 100) | dimensionless |\n",
        "| 16 | [`state`][id18] | State | dimensionless |\n",
        "| 17 | [`subcategory`][id19] | Subcategory | dimensionless |\n",
        "| 18 | [`size`][id20] | Size of Charity (based on Total Expenses) | dimensionless |\n",
        "| 19 | [`program_exp`][id21] | Program Expenses in (amount spent on program & services it delivers) | dimensionless |\n",
        "| 20 | [`fund_exp`][id22] | Funding Expenses in (amount spent on raising money) | dimensionless |\n",
        "| 21 | [`admin_exp`][id23] | Administrative Expenses in $ (amount spent on overhead, staff, meeting costs) | dimensionless |\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2.1.1 - Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033343105
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "# import opendatasets as od\n",
        "# od.download(\n",
        "#     \"https://www.kaggle.com/datasets/muratkokludataset/acoustic-extinguisher-fire-dataset\")\n",
        "\n",
        "df_charity = pd.read_csv(\"CLEAN_charity_data.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### For the propose of this project we are going to use only the `description` feature as input and the `category` as output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_charity.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Description sample\n",
        "df_charity['description'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop all features but category and description\n",
        "df_charity.drop(columns=['ascore','ein','tot_exp','admin_exp_p','fund_eff',\n",
        "'fund_exp_p', 'program_exp_p', 'fscore', 'leader',\n",
        "'leader_comp', 'leader_comp_p', 'motto', 'name', 'tot_rev', 'score',\n",
        "'state', 'subcategory', 'size', 'program_exp', 'fund_exp', 'admin_exp'], inplace = True)\n",
        "# Drop the rows with the category `international`\n",
        "df_charity = df_charity[df_charity.category != 'International']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.data_check(df_charity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.unique(df_charity['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_prod = df_charity.groupby('category').size().reset_index().rename(\n",
        "    columns={0: 'total'}).sort_values('total', ascending=False)\n",
        "fig = px.pie(top_prod, values='total', names='category', width=800, height=800,\n",
        "             color_discrete_sequence=px.colors.sequential.thermal, \n",
        "             title=\"Charity Categories\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_prod"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observations:\n",
        "1. Both `description` and `category` are object dtype\n",
        "2. There are 6 duplicated rows\n",
        "3. There are 10 individual categories (after delete the `International`) for the charities with most being `Human Services` category\n",
        "4. The plot above displays the distribution of all charity categories"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.2 - Data Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033362854
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Transform the object type into category type for the category target\n",
        "df_charity[\"category\"] = df_charity[\"category\"].astype(\"category\")\n",
        "df_charity.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033367769
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_charity[\"description_no_punctuation\"] = df_charity[\"description\"].apply(utils.remove_punctuation)\n",
        "df_charity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033391753
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_charity.drop_duplicates(inplace=True)\n",
        "df_charity.reset_index(inplace = True, drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033393806
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_charity.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033398739
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Removing Stop words\n",
        "stop_words = stopwords.words(\"english\")\n",
        "# print(sorted(stop_words))\n",
        "# print(len(stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033403216
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "func = lambda x: ' '.join([word for word in x.split() if word not in (stop_words)])\n",
        "df_charity[\"description_clean\"] = df_charity[\"description_no_punctuation\"].apply(func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666033410752
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_charity.drop([\"description_no_punctuation\"], axis = 1, inplace = True)\n",
        "df_charity.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 2.1.3 - TF-IDF with n-grams\n",
        "(Term Frequency–Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666037226829
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    lowercase = True, \n",
        "    stop_words = stop_words, \n",
        "    ngram_range = (1, 2), \n",
        "    min_df = 0.009, \n",
        "    max_df = 0.99\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(df_charity[\"description_clean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666037231000
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(\"=\"*30)\n",
        "print(f\"Number of feature names: {len(vectorizer.get_feature_names())}\")\n",
        "print(\"=\"*30)\n",
        "# The first 50 features\n",
        "print(f\"The first 50 features:\\n{np.transpose(vectorizer.get_feature_names())[:50]}\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Number of rows: {len(X.toarray())}\")\n",
        "print(\"=\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666037273126
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_charity_description_tfidf = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())\n",
        "df_charity_description_tfidf[\"category\"] = df_charity[\"category\"]\n",
        "df_charity_description_tfidf.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat = np.unique(df_charity_description_tfidf.category)\n",
        "display(cat)\n",
        "print(f\"Number of Categories: {len(cat)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1666037290942
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set all features and the target to numbers that represent each values decribed on table above.\n",
        "category_dict = {\n",
        "    'Human Services':0 ,\n",
        "    'Health' : 1, \n",
        "    'Education': 2, \n",
        "    'Arts, Culture, Humanities':3 ,\n",
        "    'Religion' : 4, \n",
        "    'Research and Public Policy': 5,\n",
        "    # 'International':6,\n",
        "    'Community Development' : 6, \n",
        "    'Animals': 7,\n",
        "    'Human and Civil Rights':8,\n",
        "    'Environment' : 9}\n",
        "\n",
        "data = df_charity_description_tfidf.replace(category_dict)\n",
        "# data.drop([\"description\", \"description_clean\"], axis=1, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = \"category\"\n",
        "features = list(data.drop(target, axis=1))\n",
        "# print(f\"Feature: \\n {features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Target: {target}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.4 - Feature Selection\n",
        "Let's reduce the amount of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X,y = data.iloc[:,:-1], data.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selector = SelectKBest(chi2, k=1118) # Reducing the features didn't improve the results\n",
        "X_new = selector.fit_transform(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = selector.get_support(indices=True)\n",
        "features_new = list(np.array(features)[cols])\n",
        "X_new = pd.DataFrame(X_new, columns=features_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = pd.concat([X, y], axis=1)\n",
        "data_new = pd.concat([X_new, y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_new = X\n",
        "# features_new = features\n",
        "# data_new = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data split\n",
        "train, test = train_test_split(\n",
        "    data_new, test_size=0.20, stratify=data_new[target], random_state=1)\n",
        "print(data_new.shape)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(train.shape[0] + test.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train[target].value_counts())\n",
        "print(test[target].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_new['category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `stratify` parameter is to keep the same proportions of the classes in the target for both train and test dataframes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.5 - Oversampling and Undersampling for inbalanced data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both training and testing data for the target are still inbalanced.\n",
        "\n",
        "There are too many moderately adapted students and too few highly-adapted students. Let's create an alternative dataset with oversampling and undersampling in case our models are not tought correctly on the original train datasets.\n",
        "\n",
        "Duplicate rows to increase class 2 (High level of adaptability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if smote:\n",
        "    oversample = SMOTE()\n",
        "    train[features_new], train[target] = oversample.fit_resample(train.drop(\n",
        "        [\"category\"], axis=1), train[\"category\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and Test set after applying oversampling\n",
        "print(train[target].value_counts())\n",
        "print(test[target].value_counts())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 - Classification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%time\n",
        "# Training model\n",
        "model_lr = LogisticRegression()\n",
        "\n",
        "# Parameters to test\n",
        "# params = {\n",
        "#     \"solver\": (\"newton-cg\", \"lbfgs\"),\n",
        "#     \"max_iter\": tuple(range(20, 35, 5)),\n",
        "#     \"class_weight\": [\"balanced\", None],\n",
        "#     'multi_class': ['auto'],\n",
        "#     'random_state': [1],\n",
        "# }\n",
        "\n",
        "# Best parameters\n",
        "params = {\n",
        "    'solver': 'newton-cg',\n",
        "    'max_iter': 8,\n",
        "    'class_weight': 'balanced',\n",
        "    'multi_class': 'auto',\n",
        "    'random_state': 1,\n",
        "}\n",
        "\n",
        "pred_lr_test = utils.results(\n",
        "    model=model_lr, \n",
        "    params=params, \n",
        "    train=train, \n",
        "    test=test, \n",
        "    features=features_new,\n",
        "    target=target,\n",
        "    metric=metric, \n",
        "    metric_name=metric_name,\n",
        "    search_params=False)\n",
        "# 0.920263\n",
        "# 0.842207"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_prob_lg = model_lr.predict_proba(test[features_new])\n",
        "utils.pr_curve(\n",
        "    model_name='Multiclass-Logistic', \n",
        "    target=test[target], \n",
        "    pred_prob=pred_prob_lg\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle.dump(model_lr, open('model_lr.pkl', 'wb')) # Save the Logistic regression model as pickle file"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.2 - Feature importance (First 10 features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lg_imp = pd.DataFrame(model_lr.coef_[0], columns = [\"Weights\"], index = features_new)\n",
        "lg_imp= lg_imp.sort_values(\"Weights\", ascending = False)\n",
        "lg_imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_importances = pd.Series(model_lr.coef_[0], index = features_new)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "logistic_importances.sort_values(ascending = False)[:10][::-1].plot.barh(ax = ax)\n",
        "\n",
        "ax.set_title(\"Feature Importances - Logistic Regression Classifier\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 - Gradient Boosting (XGBoost)\n",
        "\n",
        "[XGBClassifier](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier) from the `XGBoost` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%time\n",
        "# Training model\n",
        "model_xgb = XGBClassifier()\n",
        "\n",
        "# Parameters to test\n",
        "# params = {\n",
        "#     'max_depth': [6],\n",
        "#     'n_estimators': tuple(range(50,56,1)),\n",
        "#     'tree_method': [\"approx\"],\n",
        "#     'random_state': [1],\n",
        "#     'enable_categorical': [True],\n",
        "#     'gamma':[i/10.0 for i in range(5)],\n",
        "#     }\n",
        "\n",
        "# Best parameters\n",
        "params = {\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 52,\n",
        "    'tree_method': 'approx',\n",
        "    'random_state': 1,\n",
        "    'enable_categorical': True,\n",
        "    'gamma': 0.2,\n",
        "    'lambda': 3,\n",
        "    'alpha': 2\n",
        "    }\n",
        "\n",
        "pred_xgb_test = utils.results(\n",
        "    model=model_xgb, \n",
        "    params=params, \n",
        "    train=train, \n",
        "    test=test, \n",
        "    features=features_new,\n",
        "    target=target,\n",
        "    metric=metric, \n",
        "    metric_name=metric_name,\n",
        "    search_params=False)\n",
        "\n",
        "# 0.967592\n",
        "# 0.819756"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle.dump(model_xgb, open('model_xgb.pkl', 'wb')) # Save the XXBoost model as pickle file"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.2 - Feature importance (First 10 features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_importance(model_xgb, height=0.8, grid=False, max_num_features=10);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4 - Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr  = utils.model_metrics(test[target], pred_lr_test)  # Logistic Regression\n",
        "xgb = utils.model_metrics(test[target], pred_xgb_test) # Gradient Boosting XGBoot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge all model results into one sigle DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [lr, xgb]\n",
        "results = pd.concat(models, ignore_index=True)*100\n",
        "results.rename(index={\n",
        "    0:\"Logistic Regression\", \n",
        "    1:\"Gradient Boosting (XGBoost)\", \n",
        "    }, inplace=True)\n",
        "results.index.name = 'Models'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.style\\\n",
        "    .format('{:.2f}')\\\n",
        "    .highlight_max(color='green')\\\n",
        "    .highlight_min(color='red')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Test loading the pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lr = pickle.load(open(\"outputs/model_lr.pkl\", 'rb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 1\n",
        "description = df_charity['description'][n]\n",
        "category = df_charity['category'][n]\n",
        "\n",
        "print(\"Description:\\n\", description)\n",
        "print(\"\\nCategory: \", category)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1 - Remove ponctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "description_no_punctuation = utils.remove_punctuation(description)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2 - Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "clean_text = lambda x: ' '.join([word for word in x.split() if word not in (stop_words)])\n",
        "description_clean = clean_text(description_no_punctuation)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3 - TF-IDF with n-grams (Vectorize)\n",
        "(Term Frequency–Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    lowercase = True, \n",
        "    stop_words = stop_words, \n",
        "    ngram_range = (1, 2), \n",
        "    min_df = 0, \n",
        "    max_df = 1\n",
        ")\n",
        "\n",
        "# X = vectorizer.fit_transform(pd.Series(description_clean))\n",
        "X = vectorizer.fit_transform([description_clean])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_charity_description_tfidf = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names())\n",
        "df_charity_description_tfidf.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_all = pd.DataFrame(np.nan, index=[0], columns=features)\n",
        "df_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df_all.fillna(df_charity_description_tfidf)\n",
        "data = data.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat = model_lr.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "category_dict = {\n",
        "    0: 'Human Services',\n",
        "    1: 'Health', \n",
        "    2: 'Education',\n",
        "    3: 'Arts, Culture, Humanities',\n",
        "    4: 'Religion', \n",
        "    5: 'Research and Public Policy',\n",
        "    6: 'Community Development', \n",
        "    7: 'Animals',\n",
        "    8: 'Human and Civil Rights',\n",
        "    9: 'Environment'\n",
        "}\n",
        "\n",
        "result = category_dict[cat[0]]\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(features, end='')"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('altaml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "29dca0fee0e13f53796c4f5795ab1872b25a7b9e24ea4456b758c6b072986a6c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
